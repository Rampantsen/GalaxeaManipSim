import datetime
import uuid

from typing import Literal, Optional

import gymnasium as gym
import tyro
from loguru import logger
from pathlib import Path
import random
import numpy as np

from galaxea_sim.envs.base.bimanual_manipulation import BimanualManipulationEnv
from galaxea_sim.planners.bimanual import BimanualPlanner
from galaxea_sim.utils.data_utils import save_dict_list_to_hdf5, save_dict_list_to_json


def main(
    env_name: str,
    num_demos: int = 100,
    dataset_dir: str = "datasets",
    control_freq: int = 15,
    headless: bool = True,
    obs_mode: Literal["state", "image"] = "state",
    table_type: Literal["red", "white"] = "red",
    feature: Literal[
        "no-retry",
        "no-grasp_sample",
        "grasp_sample_only",
        "retry_only",
        "baseline",
        "traj_augmented",  
        "all",
        "test",
    ] = "all",
    tag: Literal["collected"] = "collected",
    ray_tracing: bool = True,
    seed: Optional[int] = None,  # æ·»åŠ seedå‚æ•°
):
    # è®¾ç½®å…¨å±€éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        logger.info(f"è®¾ç½®éšæœºç§å­: {seed}")

    env = gym.make(
        env_name,
        control_freq=control_freq,
        headless=headless,
        obs_mode=obs_mode,
        ray_tracing=ray_tracing,
    )
    
    # è®¾ç½®ç¯å¢ƒçš„éšæœºç§å­
    if seed is not None:
        env.seed(seed)
        logger.info(f"ç¯å¢ƒéšæœºç§å­å·²è®¾ç½®: {seed}")
    assert isinstance(env.unwrapped, BimanualManipulationEnv)
    planner = BimanualPlanner(
        urdf_path=f"{env.unwrapped.robot.name}/robot.urdf",
        srdf_path=None,
        left_arm_move_group=env.unwrapped.left_ee_link_name,
        right_arm_move_group=env.unwrapped.right_ee_link_name,
        active_joint_names=env.unwrapped.active_joint_names,
        control_freq=env.unwrapped.control_freq,
        env=env,
    )
    
    # è®¾ç½®planneråˆ°ç¯å¢ƒä¸­ï¼ˆç”¨äºgrasp_sampleçš„IKæµ‹è¯•ï¼‰
    if hasattr(env.unwrapped, 'set_planner'):
        env.unwrapped.set_planner(planner)

    save_dir = Path(dataset_dir) / env_name / table_type / feature / tag
    num_collected = 0
    num_tries = 0
    meta_info_list = []
    # è®°å½•æ”¶é›†å‚æ•°
    meta_info_list.append(
        dict(
            env_name=env_name,
            feature=feature,
            seed=seed,
            num_demos=num_demos,
        )
    )

    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨ç´¯ç§¯è¯¯å·®è·Ÿè¸ª
    # å½“featureåŒ…å«traj_augmentedæ—¶ï¼Œå¯ç”¨è¯¯å·®è·Ÿè¸ªï¼ˆç”¨äºè§¦å‘é‡æ–°è§„åˆ’ï¼‰
    enable_error_tracking = "traj_augmented" in feature or feature in ["all"] or feature in ["no-retry"] or feature in ["test"]
    if enable_error_tracking:
        planner.enable_traj_augmented_mode(True)
        logger.info("å·²å¯ç”¨è½¨è¿¹å¢å¼ºæ¨¡å¼ï¼ˆæ‰§è¡Œå™ªå£°ï¼‰ï¼šè®°å½•æ­£ç¡®actionï¼Œä½†å‘é€å¸¦å™ªå£°çš„actionç»™æœºå™¨äºº")

    while num_collected < num_demos:

        num_steps = 0
        traj = []
        info = {}
        _, rest_info = env.reset()
        
        # æ¯ä¸ªdemoå¼€å§‹æ—¶é‡ç½®ç´¯ç§¯è¯¯å·®
        if enable_error_tracking:
            planner.reset_accumulated_error()
        
        if not headless:
            env.render()
        for substep in env.unwrapped.solution():
            method, kwargs = substep
            
            result = planner.solve(
                substep,
                env.unwrapped.robot.get_qpos(),
                env.unwrapped.last_gripper_cmd,
                verbose=False,
            )
            if result is not None:
                # æ£€æŸ¥è¿”å›å€¼æ˜¯å¦æ˜¯tupleï¼ˆæœ‰æ‰§è¡Œå™ªå£°ï¼‰
                if isinstance(result, tuple):
                    executed_actions, planned_actions = result
                    
                    # è®°å½•å“ªä¸ªæ‰‹è‡‚åœ¨æ´»åŠ¨ï¼ˆä»substepçš„kwargsä¸­è·å–ï¼‰
                    from mplib.pymp import Pose
                    left_pose_in_substep = kwargs.get('left_pose')
                    right_pose_in_substep = kwargs.get('right_pose')
                    
                    # è®°å½•åŸå§‹ç›®æ ‡ä½å§¿ï¼ˆç”¨äºé‡æ–°è§„åˆ’ï¼‰
                    final_planned_action = planned_actions[-1]
                    current_qpos_temp = env.unwrapped.robot.get_qpos()
                    final_qpos = current_qpos_temp.copy()
                    final_qpos[env.unwrapped.left_arm_joint_indices] = final_planned_action[:planner.left_arm_action_dim]
                    final_qpos[env.unwrapped.right_arm_joint_indices] = final_planned_action[planner.left_arm_action_dim+1:planner.left_arm_action_dim+1+planner.right_arm_action_dim]
                    
                    # ä½¿ç”¨FKè®¡ç®—ç›®æ ‡æœ«ç«¯ä½å§¿
                    planner._fk.compute_forward_kinematics(final_qpos)
                    original_target_left = Pose(p=planner._fk.get_link_pose(0).p, q=planner._fk.get_link_pose(0).q)
                    original_target_right = Pose(p=planner._fk.get_link_pose(1).p, q=planner._fk.get_link_pose(1).q)
                    
                    # ä½¿ç”¨å¸¦å™ªå£°çš„actionæ‰§è¡Œï¼Œä½†è®°å½•æ­£ç¡®çš„action
                    i = 0
                    while i < len(executed_actions):
                        executed_action = executed_actions[i]
                        planned_action = planned_actions[i]
                        num_steps += 1
                        
                        # æ‰§è¡Œå¸¦å™ªå£°çš„action
                        obs, _, _, _, info = env.step(executed_action)
                        
                        # å¦‚æœå¯ç”¨äº†è¯¯å·®è·Ÿè¸ªï¼Œè®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®è¯¯å·®
                        if enable_error_tracking:
                            # è·å–æ‰§è¡Œåçš„å®é™…qpos
                            actual_qpos = env.unwrapped.robot.get_qpos()
                            
                            # æ„å»ºç›®æ ‡qposï¼ˆå°†planned_actionå¡«å…¥å®é™…qposï¼‰
                            target_qpos = actual_qpos.copy()
                            target_qpos[env.unwrapped.left_arm_joint_indices] = planned_action[:planner.left_arm_action_dim]
                            target_qpos[env.unwrapped.right_arm_joint_indices] = planned_action[planner.left_arm_action_dim+1:planner.left_arm_action_dim+1+planner.right_arm_action_dim]
                            
                            # ä½¿ç”¨FKè®¡ç®—æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®è¯¯å·®
                            ee_pos_error, _ = planner.compute_pose_error(target_qpos, actual_qpos)
                            
                            # åªè®°å½•æœ€å¤§çš„å•æ­¥è¯¯å·®ï¼Œè€Œä¸æ˜¯ç´¯åŠ æ‰€æœ‰è¯¯å·®
                            planner.accumulated_position_error = max(
                                planner.accumulated_position_error, 
                                ee_pos_error
                            )
                            
                            # æ¯éš”ä¸€å®šæ­¥æ•°æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§„åˆ’
                            if (i + 1) % 10 == 0 and planner.check_replan_needed():
                                # ä»å½“å‰ä½ç½®é‡æ–°è§„åˆ’åˆ°åŸç›®æ ‡ä½å§¿
                                logger.info(f"è§¦å‘é‡æ–°è§„åˆ’! å½“å‰è¯¯å·®: {planner.accumulated_position_error:.4f}m")
                                
                                current_qpos = env.unwrapped.robot.get_qpos()
                                
                                # åªç»™åŸå§‹substepä¸­æ´»åŠ¨çš„æ‰‹è‡‚ä¼ å…¥ç›®æ ‡ä½å§¿ï¼Œé™æ­¢çš„æ‰‹è‡‚ä¼ å…¥None
                                replan_target_left = original_target_left if left_pose_in_substep is not None else None
                                replan_target_right = original_target_right if right_pose_in_substep is not None else None
                                
                                print(f"ğŸ”„ é‡æ–°è§„åˆ’: left_pose={left_pose_in_substep is not None}, right_pose={right_pose_in_substep is not None}")
                                print(f"   replan_target_left={replan_target_left is not None}, replan_target_right={replan_target_right is not None}")
                                
                                replan_result = planner.replan_from_current(
                                    target_pose_left=replan_target_left,
                                    target_pose_right=replan_target_right,
                                    current_qpos=planner.sim2mplib_mapping(current_qpos),
                                    verbose=False
                                )
                                
                                if replan_result is not None:
                                    # é‡æ–°è§„åˆ’æˆåŠŸï¼Œç”Ÿæˆæ–°çš„è½¨è¿¹æ›¿æ¢å‰©ä½™éƒ¨åˆ†
                                    init_pos = np.zeros(planner.action_dim)
                                    init_pos[:planner.left_arm_action_dim] = current_qpos[env.unwrapped.left_arm_joint_indices]
                                    init_pos[planner.left_arm_action_dim] = env.unwrapped.last_gripper_cmd[0]
                                    init_pos[-planner.right_arm_action_dim-1:-1] = current_qpos[env.unwrapped.right_arm_joint_indices]
                                    init_pos[-1] = env.unwrapped.last_gripper_cmd[1]
                                    
                                    new_trajectory = planner.get_move_trajectory(init_pos, *replan_result)
                                    
                                    # æ ¹æ®åŸå§‹substepç¡®å®šæ´»åŠ¨æ‰‹è‡‚ï¼ˆåªç»™æ´»åŠ¨çš„æ‰‹è‡‚æ·»åŠ å™ªå£°ï¼‰
                                    if left_pose_in_substep is not None and right_pose_in_substep is not None:
                                        active_arm_for_replan = "both"
                                    elif left_pose_in_substep is not None:
                                        active_arm_for_replan = "left"
                                    elif right_pose_in_substep is not None:
                                        active_arm_for_replan = "right"
                                    else:
                                        active_arm_for_replan = "both"  # fallback
                                    
                                    # åº”ç”¨æ‰§è¡Œå™ªå£°åˆ°æ–°è½¨è¿¹ï¼ˆåªç»™æ´»åŠ¨çš„æ‰‹è‡‚æ·»åŠ ï¼‰
                                    new_executed, new_planned = planner.add_execution_noise(
                                        new_trajectory, active_arm=active_arm_for_replan,
                                        noise_probability=0.4, position_noise_std=0.015
                                    )
                                    
                                    # æ›¿æ¢å‰©ä½™çš„è½¨è¿¹
                                    executed_actions = new_executed
                                    planned_actions = new_planned
                                    i = 0  # é‡ç½®ç´¢å¼•
                                    planner.reset_accumulated_error()
                                    logger.info("âœ… é‡æ–°è§„åˆ’å®Œæˆï¼Œç»§ç»­æ‰§è¡Œæ–°è½¨è¿¹")
                                    continue
                                else:
                                    logger.warning("âš ï¸ é‡æ–°è§„åˆ’å¤±è´¥ï¼Œç»§ç»­æ‰§è¡ŒåŸè½¨è¿¹")
                                    planner.reset_accumulated_error()
                        
                        # åœ¨obsä¸­è®°å½•çš„æ˜¯æ­£ç¡®çš„actionï¼ˆç”¨äºè®­ç»ƒï¼‰
                        obs['upper_body_action_dict']['left_arm_joint_position_cmd'] = planned_action[:planner.left_arm_action_dim]
                        obs['upper_body_action_dict']['left_arm_gripper_position_cmd'] = np.array([planned_action[planner.left_arm_action_dim]])
                        obs['upper_body_action_dict']['right_arm_joint_position_cmd'] = planned_action[planner.left_arm_action_dim+1:planner.left_arm_action_dim+1+planner.right_arm_action_dim]
                        obs['upper_body_action_dict']['right_arm_gripper_position_cmd'] = np.array([planned_action[planner.left_arm_action_dim+1+planner.right_arm_action_dim]])
                        traj.append(obs)
                        if not headless:
                            env.render()
                        
                        i += 1  # ä¸‹ä¸€æ­¥
                else:
                    # æ­£å¸¸æ¨¡å¼
                    actions = result
                    for action in actions:
                        num_steps += 1
                        obs, _, _, _, info = env.step(action)
                        traj.append(obs)
                        if not headless:
                            env.render()
        num_tries += 1
        if info["success"]:
            save_dict_list_to_hdf5(traj, save_dir / f"demo_{num_collected}.h5")
            num_collected += 1
            meta_info = dict(
                reset_info=rest_info,
                success=info["success"],
                total_steps=num_steps,
                num_collected=num_collected,
                num_tries=num_tries,
            )
            meta_info_list.append(meta_info)
            save_dict_list_to_json(meta_info_list, save_dir / "meta_info.json")
            logger.info(
                f"Collected {num_collected} demos in {num_tries} tries. Success rate: {int(num_collected/num_tries*100)}%"
            )


if __name__ == "__main__":
    tyro.cli(main)

import numpy as np
import torch
import collections
from typing import Dict
from loguru import logger
from tqdm import tqdm

from diffusion_policy.env_runner.base_image_runner import BaseImageRunner
from diffusion_policy.policy.base_image_policy import BaseImagePolicy
from diffusion_policy.common.pytorch_util import dict_apply

# Galaxeaç¯å¢ƒå¯¼å…¥
import gymnasium as gym


class GalaxeaImageRunner(BaseImageRunner):
    """
    Galaxeaä»¿çœŸç¯å¢ƒè¯„ä¼°å™¨ï¼ˆGymnasiumå…¼å®¹ç‰ˆï¼‰
    ç®€åŒ–å®ç°ï¼Œä¸ä¾èµ–AsyncVectorEnvç­‰æ—§ç‰ˆgymå·¥å…·
    """
    
    def __init__(
        self,
        output_dir: str,
        env_name: str = "R1ProBlocksStackEasy-v0",
        n_test: int = 10,
        n_test_vis: int = 0,  # æš‚ä¸æ”¯æŒè§†é¢‘ï¼ˆé¿å…ä¾èµ–ï¼‰
        test_start_seed: int = 100000,
        max_steps: int = 300,
        n_obs_steps: int = 2,
        n_action_steps: int = 8,
        fps: int = 15,
        past_action: bool = False,
        tqdm_interval_sec: float = 1.0,
        **kwargs  # å¿½ç•¥å…¶ä»–å‚æ•°ä»¥ä¿æŒå…¼å®¹æ€§
    ):
        """
        Args:
            output_dir: è¾“å‡ºç›®å½•
            env_name: Galaxeaç¯å¢ƒåç§° (å¦‚ R1ProBlocksStackEasy-v0)
            n_test: æµ‹è¯•é›†è¯„ä¼°æ•°é‡
            n_test_vis: ä¿å­˜è§†é¢‘æ•°é‡ï¼ˆæš‚ä¸æ”¯æŒï¼‰
            test_start_seed: æµ‹è¯•é›†éšæœºç§å­
            max_steps: æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
            n_obs_steps: è§‚æµ‹å†å²æ­¥æ•°
            n_action_steps: åŠ¨ä½œé¢„æµ‹æ­¥æ•°
            fps: æ§åˆ¶é¢‘ç‡
            past_action: æ˜¯å¦ä½¿ç”¨è¿‡å»çš„åŠ¨ä½œ
            tqdm_interval_sec: è¿›åº¦æ¡æ›´æ–°é—´éš”
        """
        super().__init__(output_dir)
        
        self.env_name = env_name
        self.n_test = n_test
        self.n_test_vis = n_test_vis
        self.test_start_seed = test_start_seed
        self.max_steps = max_steps
        self.n_obs_steps = n_obs_steps
        self.n_action_steps = n_action_steps
        self.fps = fps
        self.past_action = past_action
        self.tqdm_interval_sec = tqdm_interval_sec
        
        logger.info(f"åˆå§‹åŒ–Galaxeaè¯„ä¼°å™¨ï¼ˆGymnasiumå…¼å®¹ç‰ˆï¼‰: {env_name}, n_test={n_test}")
    
    def run(self, policy: BaseImagePolicy) -> Dict:
        """
        è¿è¡Œç­–ç•¥è¯„ä¼°ï¼ˆç®€åŒ–å®ç°ï¼Œå…¼å®¹gymnasiumï¼‰
        
        Args:
            policy: è¦è¯„ä¼°çš„ç­–ç•¥
            
        Returns:
            åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ï¼ˆå…¼å®¹WandBæ ¼å¼ï¼‰
        """
        device = policy.device
        
        # æ”¶é›†æ‰€æœ‰episodeçš„ç»“æœ
        all_rewards = []
        all_lengths = []
        all_success = []
        
        logger.info(f"å¼€å§‹è¯„ä¼° {self.n_test} ä¸ªepisodes...")
        
        for episode_idx in tqdm(range(self.n_test), desc="Evaluating", mininterval=self.tqdm_interval_sec):
            seed = self.test_start_seed + episode_idx
            
            try:
                # åˆ›å»ºç¯å¢ƒ
                env = gym.make(self.env_name)
                
                # é‡ç½®ç¯å¢ƒ
                obs_dict, info = env.reset(seed=seed)
                
                # æå–è§‚æµ‹ï¼ˆGalaxeaæ ¼å¼ï¼‰
                obs_history = collections.deque(maxlen=self.n_obs_steps)
                
                episode_reward = 0.0
                episode_length = 0
                done = False
                truncated = False
                
                # åˆå§‹åŒ–è§‚æµ‹å†å²
                obs_processed = self._process_obs(obs_dict, device)
                for _ in range(self.n_obs_steps):
                    obs_history.append(obs_processed)
                
                # è¿è¡Œepisode
                with torch.no_grad():
                    while not (done or truncated) and episode_length < self.max_steps:
                        # å‡†å¤‡ç­–ç•¥è¾“å…¥ï¼ˆå †å è§‚æµ‹å†å²ï¼‰
                        obs_seq = self._stack_obs_history(obs_history)
                        
                        # é¢„æµ‹åŠ¨ä½œ
                        action_dict = policy.predict_action(obs_seq)
                        action = action_dict['action'][0].cpu().numpy()  # å–ç¬¬ä¸€æ­¥åŠ¨ä½œ
                        
                        # æ‰§è¡ŒåŠ¨ä½œ
                        obs_dict, reward, done, truncated, info = env.step(action)
                        
                        # æ›´æ–°å†å²
                        obs_processed = self._process_obs(obs_dict, device)
                        obs_history.append(obs_processed)
                        
                        episode_reward += reward
                        episode_length += 1
                
                # è®°å½•ç»“æœ
                all_rewards.append(episode_reward)
                all_lengths.append(episode_length)
                
                # æ£€æŸ¥æˆåŠŸ
                success = info.get('success', False) if isinstance(info, dict) else False
                all_success.append(1.0 if success else 0.0)
                
                logger.info(f"Episode {episode_idx+1}/{self.n_test}: "
                           f"reward={episode_reward:.2f}, length={episode_length}, success={success}")
                
                env.close()
                
            except Exception as e:
                logger.error(f"Episode {episode_idx} å¤±è´¥: {e}")
                # è®°å½•å¤±è´¥çš„episodeä¸º0
                all_rewards.append(0.0)
                all_lengths.append(0)
                all_success.append(0.0)
        
        # è®¡ç®—èšåˆæŒ‡æ ‡ï¼ˆå…¼å®¹åŸç‰ˆæ ¼å¼ï¼‰
        log_data = {
            'test/mean_score': np.mean(all_rewards),
            'test/max_reward_mean': np.mean(all_rewards),
            'test/max_reward_std': np.std(all_rewards),
            'test/success_rate': np.mean(all_success),
            'test/avg_length': np.mean(all_lengths),
        }
        
        logger.info(f"ğŸ“Š è¯„ä¼°å®Œæˆ: "
                   f"å¹³å‡å¥–åŠ±={log_data['test/mean_score']:.2f}, "
                   f"æˆåŠŸç‡={log_data['test/success_rate']:.1%}")
        
        return log_data
    
    def _process_obs(self, obs_dict: Dict, device) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†Galaxeaç¯å¢ƒçš„è§‚æµ‹æ ¼å¼
        
        Args:
            obs_dict: Galaxeaç¯å¢ƒè¿”å›çš„è§‚æµ‹å­—å…¸
            device: torch device
            
        Returns:
            å¤„ç†åçš„è§‚æµ‹å­—å…¸
        """
        # æå–upper_body_observations
        upper_body = obs_dict['upper_body_observations']
        
        # æå–ä¸‰ç›¸æœºå›¾åƒ
        img_head = torch.from_numpy(upper_body['rgb_head']).float().to(device)
        img_left = torch.from_numpy(upper_body['rgb_left_hand']).float().to(device)
        img_right = torch.from_numpy(upper_body['rgb_right_hand']).float().to(device)
        
        # HWC -> CHW, normalize to [0,1]
        img_head = img_head.permute(2, 0, 1) / 255.0
        img_left = img_left.permute(2, 0, 1) / 255.0
        img_right = img_right.permute(2, 0, 1) / 255.0
        
        # æå–çŠ¶æ€ï¼ˆ16ç»´qposï¼‰
        state = np.concatenate([
            upper_body['left_arm_joint_position'],      # 7
            upper_body['left_arm_gripper_position'],    # 1
            upper_body['right_arm_joint_position'],     # 7
            upper_body['right_arm_gripper_position'],   # 1
        ], axis=0)
        state = torch.from_numpy(state).float().to(device)
        
        return {
            'img_head': img_head,
            'img_left': img_left,
            'img_right': img_right,
            'state': state,
        }
    
    def _stack_obs_history(self, obs_history: collections.deque) -> Dict[str, torch.Tensor]:
        """
        å †å è§‚æµ‹å†å²ä¸ºåºåˆ—
        
        Args:
            obs_history: è§‚æµ‹å†å²é˜Ÿåˆ—
            
        Returns:
            å †å åçš„è§‚æµ‹å­—å…¸
        """
        # æ¯ä¸ªobsæ˜¯å•å¸§ï¼Œéœ€è¦å †å æˆåºåˆ—
        obs_list = list(obs_history)
        
        obs_seq = {
            'img_head': torch.stack([o['img_head'] for o in obs_list], dim=0),  # (T, 3, H, W)
            'img_left': torch.stack([o['img_left'] for o in obs_list], dim=0),
            'img_right': torch.stack([o['img_right'] for o in obs_list], dim=0),
            'state': torch.stack([o['state'] for o in obs_list], dim=0),  # (T, state_dim)
        }
        
        # æ·»åŠ batchç»´åº¦
        obs_seq = {k: v.unsqueeze(0) for k, v in obs_seq.items()}  # (1, T, ...)
        
        return obs_seq


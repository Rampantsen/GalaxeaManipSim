"""
æµ‹è¯•å™ªå£°è¿‡æ»¤æ•°æ®é›†çš„æ­£ç¡®æ€§

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„å¸¦å™ªå£°æ ‡è®°çš„æ•°æ®é›†
2. æµ‹è¯• NoiseFilteredLeRobotDataset æ˜¯å¦æ­£ç¡®è¿‡æ»¤
3. éªŒè¯æœ‰æ•ˆå¸§çš„ç´¢å¼•æ˜ å°„
"""

import torch
import numpy as np
from torch.utils.data import Dataset
from galaxea_sim.utils.noise_filtered_dataset import (
    NoiseFilteredLeRobotDataset,
    create_noise_filtered_dataloader
)


class MockLeRobotDataset(Dataset):
    """æ¨¡æ‹Ÿçš„ LeRobotDatasetï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, total_frames=100, noise_ratio=0.1, seed=42):
        self.total_frames = total_frames
        np.random.seed(seed)
        
        # éšæœºç”Ÿæˆå™ªå£°æ ‡è®°
        self.noise_mask = np.random.rand(total_frames) < noise_ratio
        self.noise_indices = np.where(self.noise_mask)[0].tolist()
        
        print(f"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†:")
        print(f"  - æ€»å¸§æ•°: {total_frames}")
        print(f"  - å™ªå£°å¸§æ•°: {self.noise_mask.sum()}")
        print(f"  - å™ªå£°å¸§ç´¢å¼•: {self.noise_indices[:10]}..." if len(self.noise_indices) > 10 else f"  - å™ªå£°å¸§ç´¢å¼•: {self.noise_indices}")
    
    def __len__(self):
        return self.total_frames
    
    def __getitem__(self, idx):
        if idx >= self.total_frames:
            raise IndexError(f"ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ [0, {self.total_frames})")
        
        return {
            "observation.state": torch.randn(16),  # æ¨¡æ‹Ÿstate
            "action": torch.randn(16),  # æ¨¡æ‹Ÿaction
            "is_replan_noise": torch.tensor([self.noise_mask[idx]], dtype=torch.bool),
            "frame_idx": idx,  # ç”¨äºæµ‹è¯•çš„é¢å¤–å­—æ®µ
        }


def test_basic_filtering():
    """æµ‹è¯•1ï¼šåŸºæœ¬è¿‡æ»¤åŠŸèƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯•1ï¼šåŸºæœ¬è¿‡æ»¤åŠŸèƒ½")
    print("="*60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
    base_dataset = MockLeRobotDataset(total_frames=100, noise_ratio=0.1)
    
    # åˆ›å»ºè¿‡æ»¤æ•°æ®é›†
    filtered_dataset = NoiseFilteredLeRobotDataset(base_dataset, verbose=True)
    
    # éªŒè¯é•¿åº¦
    expected_valid = (~base_dataset.noise_mask).sum()
    assert len(filtered_dataset) == expected_valid, \
        f"è¿‡æ»¤åçš„é•¿åº¦ä¸æ­£ç¡®: {len(filtered_dataset)} vs {expected_valid}"
    
    print("\nâœ… æµ‹è¯•1é€šè¿‡ï¼šæ•°æ®é›†é•¿åº¦æ­£ç¡®")


def test_no_noise_in_targets():
    """æµ‹è¯•2ï¼šç¡®ä¿é‡‡æ ·çš„å¸§ä¸åŒ…å«å™ªå£°"""
    print("\n" + "="*60)
    print("æµ‹è¯•2ï¼šç¡®ä¿action targetä¸åŒ…å«å™ªå£°å¸§")
    print("="*60)
    
    base_dataset = MockLeRobotDataset(total_frames=100, noise_ratio=0.1)
    filtered_dataset = NoiseFilteredLeRobotDataset(base_dataset, verbose=False)
    
    # éå†æ‰€æœ‰è¿‡æ»¤åçš„æ•°æ®
    noise_found = False
    for i in range(len(filtered_dataset)):
        sample = filtered_dataset[i]
        frame_idx = sample["frame_idx"]
        
        # æ£€æŸ¥è¿™ä¸€å¸§åœ¨åŸå§‹æ•°æ®é›†ä¸­æ˜¯å¦ä¸ºå™ªå£°
        if base_dataset.noise_mask[frame_idx]:
            noise_found = True
            print(f"âŒ å‘ç°å™ªå£°å¸§ {frame_idx} åœ¨è¿‡æ»¤åçš„æ•°æ®é›†ä¸­ï¼")
    
    if not noise_found:
        print(f"âœ… æµ‹è¯•2é€šè¿‡ï¼šé‡‡æ ·çš„ {len(filtered_dataset)} ä¸ªå¸§éƒ½ä¸æ˜¯å™ªå£°å¸§")
    else:
        raise AssertionError("è¿‡æ»¤å¤±è´¥ï¼šå‘ç°å™ªå£°å¸§")


def test_dataloader():
    """æµ‹è¯•3ï¼šæµ‹è¯•dataloader"""
    print("\n" + "="*60)
    print("æµ‹è¯•3ï¼šæµ‹è¯•DataLoader")
    print("="*60)
    
    base_dataset = MockLeRobotDataset(total_frames=100, noise_ratio=0.1)
    
    # ä½¿ç”¨ä¾¿æ·å‡½æ•°åˆ›å»ºdataloader
    dataloader = create_noise_filtered_dataloader(
        base_dataset,
        batch_size=8,
        shuffle=False,
        num_workers=0,  # æµ‹è¯•æ—¶ç”¨0
        verbose=False,
    )
    
    total_batches = 0
    total_samples = 0
    
    for batch in dataloader:
        total_batches += 1
        batch_size = batch["observation.state"].shape[0]
        total_samples += batch_size
        
        # æ£€æŸ¥batchä¸­çš„frame_idx
        frame_indices = batch["frame_idx"].numpy()
        
        # ç¡®ä¿æ²¡æœ‰å™ªå£°å¸§
        for idx in frame_indices:
            if base_dataset.noise_mask[idx]:
                raise AssertionError(f"Batchä¸­åŒ…å«å™ªå£°å¸§ {idx}")
    
    print(f"  - æ€»batchæ•°: {total_batches}")
    print(f"  - æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  - é¢„æœŸæ ·æœ¬æ•°: {len(dataloader.dataset)}")
    print(f"âœ… æµ‹è¯•3é€šè¿‡ï¼šDataLoaderå·¥ä½œæ­£å¸¸")


def test_index_mapping():
    """æµ‹è¯•4ï¼šéªŒè¯ç´¢å¼•æ˜ å°„"""
    print("\n" + "="*60)
    print("æµ‹è¯•4ï¼šéªŒè¯ç´¢å¼•æ˜ å°„")
    print("="*60)
    
    # åˆ›å»ºä¸€ä¸ªå°æ•°æ®é›†ä¾¿äºéªŒè¯
    base_dataset = MockLeRobotDataset(total_frames=10, noise_ratio=0.2, seed=123)
    filtered_dataset = NoiseFilteredLeRobotDataset(base_dataset, verbose=False)
    
    print("\nåŸå§‹æ•°æ®é›†æ ‡è®°:")
    for i in range(len(base_dataset)):
        is_noise = base_dataset.noise_mask[i]
        marker = "âŒ" if is_noise else "âœ“"
        print(f"  å¸§ {i}: {marker} {'(å™ªå£°)' if is_noise else '(æ­£å¸¸)'}")
    
    print(f"\nè¿‡æ»¤åçš„ç´¢å¼•æ˜ å°„:")
    for i in range(len(filtered_dataset)):
        sample = filtered_dataset[i]
        original_idx = sample["frame_idx"]
        print(f"  è¿‡æ»¤ç´¢å¼• {i} -> åŸå§‹ç´¢å¼• {original_idx}")
    
    print(f"\nâœ… æµ‹è¯•4é€šè¿‡ï¼šç´¢å¼•æ˜ å°„æ­£ç¡®")


def test_edge_cases():
    """æµ‹è¯•5ï¼šè¾¹ç•Œæƒ…å†µ"""
    print("\n" + "="*60)
    print("æµ‹è¯•5ï¼šè¾¹ç•Œæƒ…å†µ")
    print("="*60)
    
    # æƒ…å†µ1ï¼šæ²¡æœ‰å™ªå£°å¸§
    print("\nå­æµ‹è¯•5.1ï¼šæ²¡æœ‰å™ªå£°å¸§")
    base_dataset1 = MockLeRobotDataset(total_frames=50, noise_ratio=0.0)
    filtered_dataset1 = NoiseFilteredLeRobotDataset(base_dataset1, verbose=False)
    assert len(filtered_dataset1) == 50, "æ²¡æœ‰å™ªå£°æ—¶é•¿åº¦åº”è¯¥ç›¸ç­‰"
    print("  âœ… é€šè¿‡")
    
    # æƒ…å†µ2ï¼šå…¨æ˜¯å™ªå£°å¸§ï¼ˆæç«¯æƒ…å†µï¼‰
    print("\nå­æµ‹è¯•5.2ï¼šå…¨æ˜¯å™ªå£°å¸§")
    base_dataset2 = MockLeRobotDataset(total_frames=50, noise_ratio=1.0)
    filtered_dataset2 = NoiseFilteredLeRobotDataset(base_dataset2, verbose=False)
    print(f"  - è¿‡æ»¤åæ•°æ®é›†å¤§å°: {len(filtered_dataset2)}")
    assert len(filtered_dataset2) == 0, "å…¨æ˜¯å™ªå£°æ—¶åº”è¯¥ä¸ºç©º"
    print("  âœ… é€šè¿‡")
    
    # æƒ…å†µ3ï¼šå¾ˆå¤šå™ªå£°å¸§
    print("\nå­æµ‹è¯•5.3ï¼š50%å™ªå£°å¸§")
    base_dataset3 = MockLeRobotDataset(total_frames=100, noise_ratio=0.5)
    filtered_dataset3 = NoiseFilteredLeRobotDataset(base_dataset3, verbose=False)
    expected = (~base_dataset3.noise_mask).sum()
    assert len(filtered_dataset3) == expected, f"è¿‡æ»¤åé•¿åº¦ä¸å¯¹: {len(filtered_dataset3)} vs {expected}"
    print(f"  - åŸå§‹: 100å¸§, è¿‡æ»¤å: {len(filtered_dataset3)}å¸§")
    print("  âœ… é€šè¿‡")
    
    print(f"\nâœ… æµ‹è¯•5é€šè¿‡ï¼šæ‰€æœ‰è¾¹ç•Œæƒ…å†µæ­£å¸¸")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ§ª"*30)
    print("å¼€å§‹æµ‹è¯•å™ªå£°è¿‡æ»¤æ•°æ®é›†")
    print("ğŸ§ª"*30)
    
    try:
        test_basic_filtering()
        test_no_noise_in_targets()
        test_dataloader()
        test_index_mapping()
        test_edge_cases()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å™ªå£°è¿‡æ»¤æ–¹æ¡ˆå·¥ä½œæ­£å¸¸ï¼")
        print("="*60)
        
    except Exception as e:
        print("\n" + "="*60)
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("="*60)
        raise


if __name__ == "__main__":
    run_all_tests()

